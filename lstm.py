# -*- coding: utf-8 -*-
"""LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JhURBrOpDIBPNbOVL6-_V5Kkjxz9bQdG
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import itertools

from statsmodels.tsa.arima_model import ARIMA
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.stattools import acf, pacf
from statsmodels.tsa.stattools import adfuller
from sklearn.linear_model import LinearRegression
from mpl_toolkits import mplot3d

sns.set(style="whitegrid")
warnings.filterwarnings("ignore")

df = pd.read_csv('/content/Country Complexity Rankings 1995 - 2020 (3).csv', index_col = 'Date', parse_dates=True)

df.head()

df['India'].plot(figsize=(12,6))

from statsmodels.tsa.seasonal import seasonal_decompose

results = seasonal_decompose(df['India'])
results.plot()

len(df['India'])

train = df['India'].iloc[5:].values
test = df['India'].iloc[:5].values
test

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()

test.reshape((-1,1))
train.reshape((-1,1))

train = np.expand_dims(train,0)
test = np.expand_dims(test, 0)
train = train.T
test = test.T

scaler.fit(train)
scaled_train = scaler.transform(train)
scaled_test = scaler.transform(test)
scaled_train

from keras.preprocessing.sequence import TimeseriesGenerator

n_input = 17
n_features = 1
generator = TimeseriesGenerator(scaled_train, scaled_train, length = n_input, batch_size = 1)

print(len(scaled_train), len(generator))

X, y = generator[0]
X.shape

n_input = 13
generator = TimeseriesGenerator(scaled_train, scaled_train, length = n_input, batch_size =1)

from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM

model = Sequential()
model.add(LSTM(200, activation ='relu', input_shape =(n_input, n_features)))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mse')

model.summary()

model.fit(generator, epochs=950)

loss_per_epoch = model.history.history['loss']
plt.plot(range(len(loss_per_epoch)), loss_per_epoch)

last_train_batch = scaled_train[-13:]

last_train_batch = last_train_batch.reshape((1, n_input, n_features))

model.predict(last_train_batch)

scaled_test[0]

test_predictions = []

first_eval_batch = scaled_train[-n_input:]
current_batch = first_eval_batch.reshape((1, n_input, n_features))

current_batch.shape

current_batch

test_predictions = []

first_eval_batch = scaled_train[-n_input:]
current_batch = first_eval_batch.reshape((1, n_input, n_features))

for i in range(len(test)):
  current_pred = model.predict(current_batch)[0]
  test_predictions.append(current_pred)
  current_batch = np.append(current_batch[:,1:,:], [[current_pred]], axis=1)

test_predictions

true_predictions = scaler.inverse_transform(test_predictions)

test = true_predictions
test

df['India'].plot(figsize = (12,5))



